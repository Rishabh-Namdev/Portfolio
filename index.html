<!DOCTYPE html>
<html lang="en">

  <head>
    <title> Rishabh Namdev's Portfolio</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <!--<title>Agency - Start Bootstrap Theme</title>-->

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/agency.min.css" rel="stylesheet">
    <style>
      .skill-bar {
        width: 100%;
        background-color: #ddd;
        border-radius: 4px;
        margin: 5px 0;
      }

      .skill-percentage {
        height: 20px;
        border-radius: 4px;
        background-color: #337ab7;
      }

      .python-skill { width: 85%; }
      .cpp-skill { width: 75%; }
      .ros-skill { width: 80%; }

    </style>
    
  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">Rishabh Namdev's Portfolio</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav text-uppercase ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#portfolio">Projects</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#TechnicalSkills">Skills</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#team">Contact Me</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead">
      <div class="container">
        <div class="intro-text">           
          <div class="intro-heading text-uppercase"style="background-color: rgba(26, 26, 26, 0.7); color:rgba(253, 201, 43, 1); border-radius:5px;">Rishabh Namdev</div>
          <div class="intro-lead-in">"Robotics and Computer Vision: Pioneering Solutions for Real-World Challenges"</div>
          <a class="btn btn-primary btn-xl text-uppercase js-scroll-trigger" href="https://drive.google.com/file/d/1-rZr7r3vKK1trJEKIWeu9QPVdiRGKxT7/view?usp=drive_link">Resume</a>
        </div>
      </div>
    </header>
    <!-- About Me Section -->
    <section id="about" class="about-me-section">
      <div class="container">
        <div class="row">
          <div class="col-md-4 about-me-photo">
            <img src="img\about\1705996062674.jpeg" alt="Rishabh Namdev" class="img-responsive" style="width: 350px; height: auto;">
            <h2>Rishabh Namdev</h2>
            <p><strong>Profile:</strong> Robotic Engineer</p>
            <p><strong>Email:</strong> rnamdev@umich.edu</p>
            <p><strong>Phone:</strong> (703) 554-0737</p>
          </div>
          <div class="col-md-8 about-me-text">
            <h2>About me</h2>
            <p>I am Rishabh Namdev, currently pursuing a Master’s degree in Robotics Engineering from the University of Michigan-Dearborn. I hold a Bachelor’s degree in Mechanical Engineering, specializing in Mechatronics Engineering, from Medicaps University, Madhya Pradesh, India.</p>
            <p>With a robust background in technical areas including computer vision, image processing, and machine learning for robotics, I possess extensive experience in path planning, autonomous robotics, optimization techniques, multi-robot systems, and drone technology. My proficiency spans across multiple programming languages such as Python, C, and C++, and I am well-versed with MATLAB. I have hands-on experience with various frameworks and libraries related to computer vision and machine learning, including TensorFlow, PyTorch, Scikit-learn, ROS, and OpenCV.</p>
            <p>I have a track record of publishing impactful research, having presented and won the Best Presentation Award at the AIP Conference for my work on the design and fabrication of a side-mounted flapping wing mechanism in 2021. I also co-founded Indore Innovation Hub Pvt. Ltd., a robotics startup, where I led a team to develop innovative solutions in the field of robotics and secured significant government funding for our projects. Additionally, I founded and led a successful robotics team, demonstrating my leadership and management skills.</p>
            <p>I am passionate about advancing the field of robotics through innovative solutions and dedicated research, and I continuously seek opportunities to expand my knowledge and contribute to cutting-edge projects.</p>
          </div>
        </div>
      </div>
    </section>
  
    <!-- Journey Section -->
    <section id="journey" class="bg-light">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Journey</h2>
            <h3 class="section-subheading text-muted">My professional experiences and learning path.</h3>
          </div>
        </div>
        <!-- Internship 2020 -->
        <div class="row">
          <div class="col-md-4">
            <img src="img/logos/f_logo__1_-removebg-preview-e1683836513333.png" alt="Company Logo" style="width: 250px; height: auto;">
          </div>
          <div class="col-md-8">
            <h4 class="text-uppercase">Internship (January 2020 - July 2020)</h4>
            <h5><a href="https://dtownrobotics.com/" target="_blank">D-Town Robotics Pvt. Ltd.</a></h5>
            <p><strong>Role:</strong> Robotics Perception Engineer Intern.</p>
            <p><strong>What I learned:</strong>
              During my internship as a Robotics Perception Engineer at DTown Robotics Pvt. Ltd., Uttar Pradesh, India, I gained substantial experience in enhancing UAV and drone capabilities through advanced perception tools and methodologies. My work involved optimizing deep learning algorithms to improve real-time UAV responsiveness in defense missions, which significantly contributed to the efficiency and effectiveness of the projects. I also co-designed perception-driven navigation systems, resulting in a 20% improvement in drone flight path optimization.</p>
              <p>In addition to my technical contributions, this internship allowed me to collaborate with a dynamic team, further developing my problem-solving skills and understanding of practical applications in the field of robotics. The hands-on experience with cutting-edge technologies and the opportunity to work on critical projects have significantly enriched my knowledge and expertise, preparing me for future challenges in the robotics industry.</p>
          </div>
        </div>
        <!-- Robots Worked On -->
        <div class="row" style="margin-top: 20px;">
          <div class="col-md-6 text-center">
            <img src="experience\internship\pratham-2-removebg-preview.png" alt="DTR RH1NO" style="width: 90%; height: auto;">
            <h6>DTR RH1NO</h6>
          </div>
          <div class="col-md-6 text-center">
            <img src="experience\internship\Screenshot_2024-06-03_183256-removebg-preview.png" alt="TUFF D-AGRO" style="width: 60%; height: auto;">
            <h6>TUFF D-AGRO</h6>
          </div>
        </div>
      </div>
    </section>


    <!-- Indore Innovation Hub Section -->
    <section id="indore-innovation-hub" class="bg-light">
      <div class="container">
        <div class="row">
          <div class="col-md-4">
            <img src="experience\Indore innovation hub pvt ltd\Screenshot_2024-06-04_014938-removebg-preview.png" alt="Company Logo" style="width: 300px; height: auto;">
          </div>
          <div class="col-md-8">
            <h4 class="text-uppercase">Indore Innovation Hub Pvt Ltd</h4>
            <!-- <h5><a href="https://indoreinnovationhub.com/" target="_blank">Visit Company Website</a></h5> -->
            <p><strong>Role:</strong> CoFounder and Robotics CV-ML Engineer</p>
            <p><strong>Overview:</strong> Indore Innovation Hub Pvt. Ltd. was founded with the primary goal of educating the younger generation about the field of robotics. To achieve this, we partnered with the government to set up robotics labs in schools and colleges in our state and neighboring regions. These labs provided students with hands-on experience and practical knowledge, fostering a deep interest in robotics and technology.</p>
              <p>After successfully streamlining the lab setup process, we expanded our focus to developing advanced drone technology. We initially built drones for conducting surveys of chimneys and power plants, providing efficient and accurate data collection solutions. Subsequently, we leveraged our expertise to create drones for agricultural applications, offering innovative solutions for precision farming and crop monitoring. Through these initiatives, Indore Innovation Hub has contributed significantly to technological advancements and education in the region.</p>
          </div>
        </div>
        <!-- Projects -->
        <div class="row" style="margin-top: 20px;">
          <div class="col-md-6 text-center">
            <img src="experience\Indore innovation hub pvt ltd\lab.png" alt="Robotics Labs Setup" style="width: 50%; height: auto;">
            <h6>Robotics Labs Setup in Schools and Colleges</h6>
            <p>We set up state-of-the-art robotics labs in various schools and colleges, providing students with hands-on experience in robotics and STEM education.</p>
          </div>
          <div class="col-md-6 text-center">
            <img src="experience\Indore innovation hub pvt ltd\drones-03-00034-g0A2-550-removebg-preview.png" alt="Drone Surveys" style="width: 65%; height: auto;">
            <h6>Drones for Surveys</h6>
            <p>We developed drones for conducting surveys, primarily focusing on chimneys, pipelines, and agricultural surveys, bringing innovative solutions to these industries.</p>
          </div>
        </div>
      </div>
    </section>

    <!-- Portfolio Grid -->
    <section class="bg-light" id="portfolio">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Projects</h2>
            <h3 class="section-subheading text-muted"></h3>
          </div>
        </div>
        <div class="row">
          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#visuallyImpaired">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img\portfolio\Visually Impaired Navigation Assistance\VisuallyImpairedNavigationAssistance_TowardsaMoreAccessibleWorld-ezgif.com-video-to-gif-converter.gif" alt="" style="width: 100%; height: 200px; object-fit: cover;">
            </a>
            <div class="portfolio-caption">
              <h4>Visually Impaired Navigation Assistance</h4>
              <!-- <p class="text-muted">Project details and technologies used.</p> -->
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#semanticSegmentation">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img\portfolio\sementic segma=entation for off-road vehucle\rc_car_djicam2_mobilenetv3_60epochs-ezgif.com-video-to-gif-converter.gif" alt="" style="width: 100%; height: 200px; object-fit: cover;">
            </a>
            <div class="portfolio-caption">
              <h4>Semantic Segmentation for Off Road Vehicle</h4>
              <!-- <p class="text-muted">Project details and technologies used.</p> -->
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#bodyPoseEstimation">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img\portfolio\body pose estimation\Application2-ezgif.com-video-to-gif-converter (1).gif" alt="" style="width: 100%; height: 200px; object-fit: cover;">
            </a>
            <div class="portfolio-caption">
              <h4>Body Pose Estimation</h4>
              <!-- <p class="text-muted">Project details and technologies used.</p> -->
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#adaptiveRoboticSystem">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img\portfolio\Adaptive\Proj2-ezgif.com-video-to-gif-converter (1).gif" alt="" style="width: 100%; height: 200px; object-fit: cover;">
            </a>
            <div class="portfolio-caption">
              <h4>Adaptive Robotic System for Warehouse Floor Obstacle Identification</h4>
              <!-- <p class="text-muted">Project details and technologies used.</p> -->
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#pathMasterRRTLiDAR">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img\portfolio\Path Master Buttler RRT Lidar Integration\Media1-ezgif.com-video-to-gif-converter.gif" alt="" style="width: 100%; height: 200px; object-fit: cover;">
            </a>
            <div class="portfolio-caption">
              <h4>Path Master-Butler-RRT-LiDAR-Integration</h4>
              <!-- <p class="text-muted">Project details and technologies used.</p> -->
            </div>
          </div>
        </div>
      </div>
    </section>


    <!-- Research Paper Section -->
    <section id="research" class="bg-light">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Research Paper</h2>
            <h3 class="section-subheading text-muted">Published in AIP</h3>
          </div>
        </div>
        <div class="row">
          <div class="col-md-6">
            <h4 class="text-uppercase">Design and Fabrication of Side Mounted Crank Flapping Wing Robot</h4>
            <p><a href="https://pubs.aip.org/aip/acp/article-abstract/2413/1/020002/2821853/Design-and-fabrication-of-side-mounted-crank?redirectedFrom=fulltext" target="_blank">Read Full Paper</a></p>
            <p><strong>Description:</strong> This paper presents the design and fabrication process of a side-mounted crank flapping wing robot, also known as an ornithopter. The research focuses on the mechanical design, aerodynamic performance, and control mechanisms of the ornithopter. The results demonstrate the potential applications of such robots in various fields, including surveillance, environmental monitoring, and recreational activities.</p>
          </div>
          <div class="col-md-6 text-center">
            <iframe width="100%" height="315" src="https://www.youtube.com/embed/Q6PurjvhpFU" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
          </div>
        </div>
        <div class="row" style="margin-top: 20px;">
          <div class="col-md-6 text-center">
            <img src="experience\research paper\Untitledvideo-MadewithClipchamp2-ezgif.com-video-to-gif-converter.gif" alt="Ornithopter Image 1" style="width: 100%; height: auto;">
            <h6>Ornithopter Image 1</h6>
          </div>
          <div class="col-md-6 text-center">
            <img src="experience\research paper\ezgif.com-animated-gif-maker.gif" alt="Ornithopter Image 2" style="width: 100%; height: auto;">
            <h6>Ornithopter Image 2</h6>
          </div>
        </div>
      </div>
    </section>


    <!-- Team Xenon Section -->
    <section id="team-xenon" class="bg-light">
      <div class="container">
        <div class="row">
          <div class="col-md-8">
            <h2 class="text-uppercase">Team Xenon</h2>
            <h3 class="section-subheading text-muted">National and International Robotics Competitions</h3>
            <p><strong>Description:</strong> Team Xenon is a renowned robotics team known for its participation in a wide array of national and international competitions across India and neighboring countries. Our team excels in events such as Robowar in every weight category, drone racing and endurance, line follower, pick and place robots, RoboRace, maze solver, and Robo Soccer. We have garnered numerous awards and accolades for our innovative robotic solutions, showcasing our expertise and dedication in the field of robotics.</p>

              <p>Our significant achievements include winning six prestigious international competitions held at Nepal HydroCon, IIT Bombay Techfest, and TechnoXian in Delhi. These victories highlight our commitment to excellence and innovation. Through our participation in diverse competitions, we aim to inspire the next generation of robotics enthusiasts and contribute to the advancement of the field.</p>
            <div class="row" style="margin-top: 20px;">
              <div class="col-md-3 text-center">
                <img src="experience\team xenon\1717538696017-ezgif.com-video-to-gif-converter.gif" alt="Robot 1" style="width: 100%; height: auto;">
                <h6>Maharana 120lbs</h6>
              </div>
              <div class="col-md-3 text-center">
                <img src="experience\team xenon\1717539168450-ezgif.com-video-to-gif-converter.gif" alt="Robot 2" style="width: 100%; height: auto;">
                <h6>Ranasanga 60lbs</h6>
              </div>
              <div class="col-md-3 text-center">
                <img src="experience\team xenon\WhatsApp Image 2024-06-04 at 17.13.27_fbd84d2e.jpg" alt="Robot 3" style="width: 100%; height: auto;">
                <h6>Drone</h6>
              </div>
              <div class="col-md-3 text-center">
                <img src="experience\team xenon\image.png" alt="Robot 4" style="width: 100%; height: auto;">
                <h6>Modular Robot</h6>
              </div>
              
              
            </div>
          </div>
          <div class="col-md-4">
            <img src="experience\team xenon\channels4_profile-removebg-preview.png" alt="Team Xenon Logo" style="width: 40%; height: auto;">
            <iframe width="100%" height="315" src="https://www.youtube.com/embed/PuAHzLKzJmY" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            <h6>Robot Competition Video 1</h6>
            <iframe width="100%" height="315" src="https://www.youtube.com/embed/8xC4rIoTcmE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            <h6>Robot Competition Video 2</h6>
          </div>
        </div>
      </div>
    </section>
    
    



    <!-- Technical Skills Section -->
    <section id="TechnicalSkills" class="bg-light">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Technical Skills</h2>
            <h3 class="section-subheading text-muted">A comprehensive showcase of my expertise</h3>
          </div>
        </div>
        <div class="row" style="margin-top: 20px;">
          <div class="col-md-6">
            <h4 class="text-uppercase">Robotics Engineer</h4>
            <ul class="list-unstyled">
              <li><strong>Computer Vision:</strong> OpenCV, TensorFlow, Keras</li>
              <li><strong>Machine Learning:</strong> Scikit-learn, PyTorch</li>
              <li><strong>Motion Planning:</strong> RRT, PRM, Dijkstra's Algorithm</li>
              <li><strong>Robot Operating System (ROS):</strong> ROS 1, ROS 2, Gazebo</li>
              
              <li><strong>Control Systems:</strong> PID, State Space Control</li>
              <li><strong>Robotic Manipulation:</strong> Kinematics, Dynamics, Grasping</li>
            </ul>
          </div>
          <div class="col-md-6">
            <h4 class="text-uppercase">Mechatronics Engineering</h4>
            <ul class="list-unstyled">
              <li><strong>Mechanical Skills:</strong> CAD Design (SolidWorks, Onshape), Material Stress & Strength Analysis</li>
              <li><strong>Electrical Skills:</strong> Circuit Design, PCB Layout, Microcontrollers (Arduino, Raspberry Pi)</li>
              <li><strong>Manufacturing Skills:</strong> Laser Cutting, 3D Printing, CNC Machining, Soldering</li>
            </ul>
          </div>
        </div>
        <div class="row" style="margin-top: 20px;">
          <div class="col-md-6">
            <h4 class="text-uppercase">Programming</h4>
            <ul class="list-unstyled">
              <li><strong>Languages:</strong> Python, C, C++, MATLAB</li>
              <li><strong>Version Control:</strong> Git, GitHub, Docker</li>
            </ul>
          </div>
          <div class="col-md-6">
            <h4 class="text-uppercase">Additional Skills</h4>
            <ul class="list-unstyled">
              
              <li><strong>Data Analysis:</strong> Pandas, NumPy, Matplotlib</li>
              <li><strong>Communication:</strong> Technical Writing, Public Speaking</li>
            </ul>
          </div>
        </div>
      </div>
    </section>



    
    
    <!-- contact -->
    <section class="bg-light" id="team">
      <div class="container">
        <div class="row">
            <div class="col-lg-12 max-auto mb-5">
                <img class="mx-auto d-block rounded-circle" src="img\about\1705996062674.jpeg" alt="Avatar" width="300" height="300">
            </div>
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Contact me</h2>
           
          </div>
        </div>
        <div class="row">
          <div class="col-lg-12">
         

            <div class="team-member">
              
              <h4>Rishabh Namdev</h4>
              <p class="text-muted">Master of Science in Robotics Enginnering @ University of Michigan</p>
              <ul class="list-inline social-buttons">
                <li class="list-inline-item">
                  <a href="https://github.com/Rishabh-Namdev">
                    <i class="fab fa-github"></i>
                  </a>
                </li>
                <li class="list-inline-item">
                  <a href="mailto:rnamdev@umich.edu">
                    <i class="fa fa-envelope"></i>
                  </a>
                </li>
                <li class="list-inline-item">
                  <a href="https://www.linkedin.com/in/rishabh-namdev-71b44a294/">
                    <i class="fab fa-linkedin-in"></i>
                  </a>
                </li>
              </ul>
            </div>
          </div>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-8 mx-auto text-center">
           
          </div>
        </div>
      </div>
    </section>


        
        
        
        <!-- Portfolio Modals -->

      <!-- Visually Impaired Navigation Assistance Modal -->
      <div class="portfolio-modal modal fade" id="visuallyImpaired" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="close-modal" data-dismiss="modal">
                    <div class="lr">
                        <div class="rl"></div>
                    </div>
                </div>
                <div class="container">
                    <div class="row">
                        <div class="col-lg-8 mx-auto">
                            <div class="modal-body">
                                <!-- Project Details Go Here -->
                                <h2 class="text-uppercase">Visually Impaired Navigation Assistance</h2>
                                
                                <div class="tags">
                                  <span class="badge badge-pill badge-warning">Deep Learning</span>
                                  <span class="badge badge-pill badge-warning">Tensorflow</span>
                                  <span class="badge badge-pill badge-warning">Python</span>
                                  <span class="badge badge-pill badge-warning">Computer Vision</span>
                                  <span class="badge badge-pill badge-warning">LLM</span>
                               </div>
                               <p class="item-intro text-muted">Empowering visually impaired individuals through real-time scene description technology.</p>
                                
                                <!-- Brief Description -->
                                <h3>Brief Description</h3>
                                <p>This research introduces a transformative approach to empower visually impaired individuals in navigating their surroundings independently. Utilizing the innovative ExpansionNet v2 model, this system provides real-time, dynamic audio descriptions of scenes, significantly enhancing mobility both indoors and outdoors with an impressive 85% accuracy.</p>

                                <!-- Image Example -->
                                <img class="img-fluid d-block mx-auto" src="img\portfolio\Visually Impaired Navigation Assistance\example expansion net vs cnn.png" alt="Visually Impaired Navigation Assistance Example" />

                                <!-- Proposed Model -->
                                <h3>Proposed CNN Model</h3>
                                <p>ExpansionNet v2 utilizes a modified Vision Transformer (ViT) architecture as its backbone. The model is trained using a two-stage approach:</p>
                                <ul>
                                    <li><strong>Stage 1: Cross-entropy pre-training:</strong> Trained on both the original image and its expanded blocks, with captions generated from them.</li>
                                    <li><strong>Stage 2: Reinforcement learning fine-tuning:</strong> BLEU score guides the model’s learning through reinforcement learning, generating fluent, grammatically correct, and engaging captions.</li>
                                </ul>

                                <!-- Detailed Description with Photos -->
                                <h3>Detailed Description</h3>
                                <p>Unlike conventional models, our approach leverages transformers and heterogeneous sequence processing, ensuring a nuanced understanding of complex scenes. By incorporating real-time video streams, it allows for immediate and accurate scene interpretation. This model demonstrates robust generalization even under less favorable conditions, using datasets like Microsoft COCO and nocaps.</p>

                                <img class="img-fluid d-block mx-auto" src="img\portfolio\Visually Impaired Navigation Assistance\architechture.png" alt="Model Architecture: ExpansionNet v2 for Image Captioning" />
                                <p><em>Figure 1: Model Architecture: ExpansionNet v2 for Image Captioning</em></p>

                                <img class="img-fluid d-block mx-auto" src="img\portfolio\Visually Impaired Navigation Assistance\image.png" alt="Training Process" />
                                <p><em>Figure 2: Stats</em></p>

                                <!-- Demo Video -->
                                <!-- <h3>Demo Video</h3>
                                <iframe width="560" height="315" src="https://youtu.be/embed/5UDDeCgmxmE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->

                                <h3>Demo Video</h3>
                                <iframe width="560" height="315" src="https://www.youtube.com/embed/5UDDeCgmxmE" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


                                <!-- Link to Full Report -->
                                <h3>Read Full Report</h3>
                                <p>For more details, please refer to the full report: <a href="link_to_your_full_report" target="_blank">Visually Impaired Navigation Assistance: Towards a More Accessible World</a></p>

                                <button class="btn btn-primary" data-dismiss="modal" type="button">
                                    <i class="fas fa-times"></i>
                                    Close Project
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
      </div>

      <!-- Semantic Segmentation for Off-Road Vehicles Modal -->
      <div class="portfolio-modal modal fade" id="semanticSegmentation" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="close-modal" data-dismiss="modal">
                    <div class="lr">
                        <div class="rl"></div>
                    </div>
                </div>
                <div class="container">
                    <div class="row">
                        <div class="col-lg-8 mx-auto">
                            <div class="modal-body">
                                <!-- Project Details Go Here -->
                                <h2 class="text-uppercase">Semantic Segmentation for Off-Road Vehicles</h2>
                                <!-- Tags -->
                                <div class="tags">
                                    <span class="badge badge-pill badge-warning">Deep Learning</span>
                                    <span class="badge badge-pill badge-warning">Tensorflow</span>
                                    <span class="badge badge-pill badge-warning">Python</span>
                                    <span class="badge badge-pill badge-warning">Computer Vision</span>
                                    <span class="badge badge-pill badge-warning">MobileNet</span>
                                </div>
                                <p class="item-intro text-muted">Enhancing off-road navigation for autonomous vehicles using semantic segmentation.</p>
                                
                                <!-- Brief Description -->
                                <h3>Brief Description</h3>
                                <p>This project addresses the challenges of off-road navigation by employing semantic segmentation using Deep Learning to discern and navigate off-road terrains for autonomous and semi-autonomous vehicles. Utilizing the DeepLabv3 architecture with a MobileNetV3 backbone, our model segments off-road scenes into navigable paths, obstacles, and non-traversable areas, enhancing the vehicle's situational awareness and decision-making capabilities.</p>

                                <!-- Image Example -->
                                <img class="img-fluid d-block mx-auto" src="img\portfolio\sementic segma=entation for off-road vehucle\example.png" alt="Semantic Segmentation for Off-Road Vehicles Example" />

                                <!-- Technologies Used -->
                                <h3>Technologies Used</h3>
                                <p>Our model leverages the DeepLabv3 architecture with a MobileNetV3 backbone, optimized for real-time applications. Key technologies and methods include:</p>
                                <ul>
                                    <li><strong>DeepLabv3:</strong> For state-of-the-art semantic segmentation.</li>
                                    <li><strong>MobileNetV3:</strong> An efficient CNN architecture optimized for mobile and edge devices.</li>
                                    <li><strong>Yamaha-CMU Off-Road Dataset:</strong> A comprehensive collection of labeled images from diverse off-road environments.</li>
                                    <li><strong>Cross-Entropy Loss:</strong> For training the segmentation model effectively.</li>
                                </ul>

                                <!-- Contribution -->
                                <h3>My Contribution</h3>
                                <p>As a key member of the project team, my responsibilities included data loading and preprocessing, as well as adapting the MobileNet architecture to our specific requirements. I ensured the seamless integration of the dataset into our model and optimized the architecture to balance computational efficiency with high accuracy. My efforts were pivotal in achieving real-time segmentation performance, making the system suitable for off-road vehicle applications.</p>

                                <!-- Detailed Description with Photos -->
                                <h3>Detailed Description</h3>
                                <p>Our project employed DeepLabv3, a cutting-edge semantic segmentation architecture that extends CNN capabilities through spatial pyramid pooling. This allows the model to capture complex structures within images at multiple scales. MobileNetV3, our chosen backbone, balances computational efficiency and performance, making it ideal for real-time applications.</p>

                                <img class="img-fluid d-block mx-auto" src="img\portfolio\sementic segma=entation for off-road vehucle\architechture.png" alt="Model Architecture" />
                                <p><em>Figure 1: Model Architecture</em></p>

                                <img class="img-fluid d-block mx-auto" src="img\portfolio\sementic segma=entation for off-road vehucle\training.png" alt="Training Process" />
                                <p><em>Figure 2: Training Process</em></p>

                                <!-- Demo Video -->
                                <h3>Demo Video</h3>
                                <iframe width="560" height="315" src="https://www.youtube.com/embed/EmA6A0_CJuc" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

                                <!-- Link to Full Report -->
                                <h3>Read Full Report</h3>
                                <p>For more details, please refer to the full report: <a href="https://drive.google.com/file/d/1dKpFYaTfyYPqSCnkNshSCJow-VXh0zrG/view?usp=drive_link" target="_blank">Semantic Segmentation for Off-Road Vehicles</a></p>

                                <button class="btn btn-primary" data-dismiss="modal" type="button">
                                    <i class="fas fa-times"></i>
                                    Close Project
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
      </div>


      <!-- Body Pose Estimation -->
      <!-- Body Pose Estimation Modal -->
      <div class="portfolio-modal modal fade" id="bodyPoseEstimation" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="close-modal" data-dismiss="modal">
                    <div class="lr">
                        <div class="rl"></div>
                    </div>
                </div>
                <div class="container">
                    <div class="row">
                        <div class="col-lg-8 mx-auto">
                            <div class="modal-body">
                                <!-- Project Details Go Here -->
                                <h2 class="text-uppercase">Body Pose Estimation</h2>
                                <!-- Tags -->
                                <div class="tags">
                                    <span class="badge badge-pill badge-warning">Deep Learning</span>
                                    <span class="badge badge-pill badge-warning">Tensorflow</span>
                                    <span class="badge badge-pill badge-warning">Python</span>
                                    <span class="badge badge-pill badge-warning">Computer Vision</span>
                                    <span class="badge badge-pill badge-warning">MobileNet</span>
                                </div>
                                <p class="item-intro text-muted">Developing a deep learning-based approach for human pose estimation using the COCO dataset.</p>
                                
                                <!-- Brief Description -->
                                <h3>Brief Description</h3>
                                <p>This project focuses on developing a deep learning-based approach for human pose estimation using the popular COCO dataset. Leveraging the advanced capabilities of convolutional neural networks (CNNs), the project implements a pose estimation model that accurately identifies and tracks human body keypoints in images. The core of the project is built around a tailored architecture based on the PoseEstimationWithMobileNet model, known for its efficiency and accuracy in processing images for keypoint detection. The project's outcome demonstrates the model's proficiency in detecting human poses with high accuracy. The results are quantified using standard metrics and visually represented to show the model's effectiveness. This report encapsulates the journey from conceptualization to implementation, offering insights into the challenges faced and the innovative solutions adopted. The project sets a foundation for future enhancements in real-time applications and more complex pose estimation challenges.</p>

                                <!-- Image Example -->
                                <img class="img-fluid d-block mx-auto" src="img\portfolio\body pose estimation\example.png" alt="Body Pose Estimation Example" />

                                <!-- Technologies Used -->
                                <h3>Technologies Used</h3>
                                <p>Our model leverages the PoseEstimationWithMobileNet architecture, optimized for real-time applications. Key technologies and methods include:</p>
                                <ul>
                                    <li><strong>PoseEstimationWithMobileNet:</strong> For efficient and accurate keypoint detection.</li>
                                    <li><strong>COCO Dataset:</strong> A comprehensive dataset with extensive annotations for human keypoints.</li>
                                    <li><strong>Python and PyTorch:</strong> For developing and training the neural network model.</li>
                                    <li><strong>L2 Loss Function and Adam Optimizer:</strong> To ensure robust and generalizable model training.</li>
                                </ul>

                                <!-- Contribution -->
                                <h3>My Contribution</h3>
                                <p>As a key member of the project team, my responsibilities included data loading and preprocessing, as well as adapting the PoseEstimationWithMobileNet architecture to our specific requirements. I ensured the seamless integration of the dataset into our model and optimized the architecture to balance computational efficiency with high accuracy. My efforts were pivotal in achieving high-accuracy pose estimation performance, making the system suitable for real-time applications.</p>

                                <!-- Detailed Description with Photos -->
                                <h3>Detailed Description</h3>
                                <p>This was my solo project, given by my professor to improve and optimize a preexisting 2017 model. I increased the depthwise separable and refinement layers to enhance keypoint detection and localization. However, the model initially suffered from overfitting. To address this, I implemented the ADAM optimizer and L2 loss regularization, which significantly improved the model's performance. Initially running at 30 fps, my optimizations increased the model's frame rate to 49 fps.</p>

                                <img class="img-fluid d-block mx-auto" src="img\portfolio\body pose estimation\architechture.png" alt="Model Architecture" />
                                <p><em>Figure 1: Model Architecture: Open Pose</em></p>

                                <!-- <img class="img-fluid d-block mx-auto" src="img/portfolio/body_pose_estimation_3.jpg" alt="Training Process" />
                                <p><em>Figure 2: Training Process</em></p> -->

                                <!-- Demo Video -->
                                <h3>Demo Videos</h3>
                                <div class="row">
                                    <div class="col-md-4">
                                        <iframe width="100%" height="200" src="https://www.youtube.com/embed/0FMf3zcR9Dg" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                    <div class="col-md-4">
                                        <iframe width="100%" height="200" src="https://www.youtube.com/embed/3DEajb-X19k" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                    <div class="col-md-4">
                                        <iframe width="100%" height="200" src="https://www.youtube.com/embed/tXY6QN4JZ5o" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                                    </div>
                                </div>
                                <!-- Link to Full Report -->
                                <h3>Read Full Report</h3>
                                <p>For more details, please refer to the full report: <a href="link_to_your_full_report" target="_blank">Body Pose Estimation using Deep Learning</a></p>

                                <button class="btn btn-primary" data-dismiss="modal" type="button">
                                    <i class="fas fa-times"></i>
                                    Close Project
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
      </div>


      <!-- Adaptive Robotic System for Warehouse Floor Obstacle Identification -->
      <!-- Adaptive Robotic System Modal -->
      <div class="portfolio-modal modal fade" id="adaptiveRoboticSystem" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="close-modal" data-dismiss="modal">
                    <div class="lr">
                        <div class="rl"></div>
                    </div>
                </div>
                <div class="container">
                    <div class="row">
                        <div class="col-lg-8 mx-auto">
                            <div class="modal-body">
                                <!-- Project Details Go Here -->
                                <h2 class="text-uppercase">Adaptive Robotic System for Warehouse Floor Obstacle Identification</h2>
                                <!-- Tags -->
                                <div class="tags">
                                    <span class="badge badge-pill badge-warning">Obstacle Detection</span>
                                    <span class="badge badge-pill badge-warning">Obstacle Recognition</span>
                                    <span class="badge badge-pill badge-warning">YOLO</span>
                                    <span class="badge badge-pill badge-warning">Real-time Applications</span>
                                    <span class="badge badge-pill badge-warning">Computer Vision</span>
                                </div>
                                <p class="item-intro text-muted">Developing an integrated obstacle detection and recognition system using YOLO for real-time applications in a warehouse setting.</p>

                                <!-- Brief Description -->
                                <h3>Brief Description</h3>
                                <p>Obstacle detection and recognition are critical tasks in robotics and automation, ensuring safe and efficient operation across various industries. In this project, we propose the development of an integrated obstacle detection and recognition system using a customized Obstacle Detection pipeline. The system utilizes advanced algorithms, such as YOLO (You Only Look Once), to detect and identify obstacles in real-time within indoor environments. By combining custom obstacle detection and recognition techniques, the mobile robot will be able to not only detect obstacles but also accurately classify and recognize them. The project aims to leverage computer vision and the creation of a custom dataset to enhance the robot’s perception capabilities, enabling it to navigate complex environments effectively and avoid potential hazards.</p>

                                <!-- Image Example -->
                                <img class="img-fluid d-block mx-auto" src="img\portfolio\Adaptive\123.png" alt="Obstacle Detection Example" />

                                <!-- Technologies Used -->
                                <h3>Technologies Used</h3>
                                <p>Our project employs the YOLO algorithm for real-time obstacle detection and recognition. Key technologies and methods include:</p>
                                <ul>
                                    <li><strong>YOLO Algorithm:</strong> For real-time object detection and recognition.</li>
                                    <li><strong>Custom Datasets:</strong> Annotated bounding box coordinates and class labels for diverse obstacles.</li>
                                    <li><strong>Raspberry Pi and Sensors:</strong> For hardware implementation of the robot.</li>
                                    <li><strong>Python and OpenCV:</strong> For developing and integrating the detection system.</li>
                                </ul>

                                <!-- Contribution -->
                                <!-- <h3>My Contribution</h3>
                                <p>This was my solo project, assigned by my professor to improve and optimize a preexisting model. I implemented the YOLO algorithm, increased the depthwise separable and refinement layers to enhance keypoint detection, and applied the ADAM optimizer and L2 loss regularization to improve the model's frame rate. These changes boosted the model's performance from 30 fps to 49 fps.</p> -->

                                <!-- Detailed Description with Photos -->
                                <h3>Detailed Description</h3>
                                <p>The hardware implementation involves a differential drive system allowing the robot to navigate and control its motion by independently controlling the speed and direction of its two wheels. The robot consists of a chassis, four motorized wheels, a Raspberry Pi for running the obstacle avoidance algorithm, an onboard microcontroller for IMU data, and various sensors for perception and environment interaction. The control loop and experimental results demonstrate the successful implementation of the differential drive system, with the robot accurately following predefined paths and achieving consistent odometry estimation.</p>

                                <img class="img-fluid d-block mx-auto" src="img\portfolio\Adaptive\image.png" alt="Robot Design" />
                                <p><em>Figure 1: Robot Design</em></p>

                                <!-- <img class="img-fluid d-block mx-auto" src="img/portfolio/warehouse_robot_3.jpg" alt="Control Loop" />
                                <p><em>Figure 2: Control Loop</em></p> -->

                                <!-- Demo Videos -->
                                <h3>Demo Video</h3>
                                <iframe width="560" height="315" src="https://www.youtube.com/embed/Fbmvh2RHV_U" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


                                <!-- Link to Full Report -->
                                <h3>Read Full Report</h3>
                                <p>For more details, please refer to the full report: <a href="https://drive.google.com/file/d/1TGJPLtSz6UVgddRyB8xiLwSJV3QjoHHi/view?usp=drive_link" target="_blank">Adaptive Robotic System for Warehouse Floor Obstacle Identification</a></p>

                                <button class="btn btn-primary" data-dismiss="modal" type="button">
                                    <i class="fas fa-times"></i>
                                    Close Project
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
      </div>


      <!-- Path Master-Butler-RRT-LiDAR-Integration Modal -->
      <div class="portfolio-modal modal fade" id="pathMasterRRTLiDAR" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
                <div class="close-modal" data-dismiss="modal">
                    <div class="lr">
                        <div class="rl"></div>
                    </div>
                </div>
                <div class="container">
                    <div class="row">
                        <div class="col-lg-8 mx-auto">
                            <div class="modal-body">
                                <!-- Project Details Go Here -->
                                <h2 class="text-uppercase">Path Master-Butler-RRT-LiDAR-Integration</h2>
                                <!-- Tags -->
                                <div class="tags">
                                    <span class="badge badge-pill badge-warning">Motion Planning</span>
                                    <span class="badge badge-pill badge-warning">RRT Algorithm</span>
                                    <span class="badge badge-pill badge-warning">Bidirectional Search</span>
                                    <span class="badge badge-pill badge-warning">Robotics</span>
                                    <span class="badge badge-pill badge-warning">SLAM</span>
                                </div>
                                <p class="item-intro text-muted">Optimized Double Tree RRT* algorithm for efficient motion planning in complex environments, integrated with SLAM for real-time navigation.</p>

                                <!-- Brief Description -->
                                <h3>Brief Description</h3>
                                <p>Motion planning is crucial for enabling robotics and autonomous systems to navigate complex environments. The Rapidly Exploring Random Trees (RRT) algorithm has proven to be effective in finding feasible paths but suffers from slow convergence in large configuration spaces or when start and goal configurations are distant. In this project, we propose an optimized version of Double Tree RRT* algorithm, an extension of RRT* that incorporates a bidirectional double-tree approach to enhance efficiency. The Double Tree RRT* algorithm maintains two separate trees, a forward tree growing from the start configuration and a backward tree growing from the goal configuration. These trees simultaneously explore the configuration space using random sampling and extension operations.</p>

                                <!-- Image Example -->
                                <img class="img-fluid d-block mx-auto" src="img\portfolio\Path Master Buttler RRT Lidar Integration\image.png" alt="Double Tree RRT* Example" />

                                <!-- Technologies Used -->
                                <h3>Technologies Used</h3>
                                <p>Our project employs the following technologies:</p>
                                <ul>
                                    <li><strong>Double Tree RRT* Algorithm:</strong> For efficient bidirectional motion planning.</li>
                                    <li><strong>SLAM (Simultaneous Localization and Mapping):</strong> For creating accurate maps and estimating the robot's pose.</li>
                                    <li><strong>LIDAR:</strong> For capturing detailed environmental data.</li>
                                    <li><strong>ROS (Robot Operating System):</strong> For robot simulation and control.</li>
                                    <li><strong>Python and Gazebo:</strong> For developing and testing the path planning system.</li>
                                </ul>

                                <!-- Contribution -->
                                <!-- <h3>My Contribution</h3>
                                <p>This was my solo project, where I was responsible for preparing and training the custom dataset for object detection. I used the Roboflow website for classifying and annotating the custom dataset and created a YAML file. Due to the limited size of our dataset, I performed image augmentation to enhance the dataset and improve the model's robustness. Additionally, I implemented the optimized Double Tree RRT* algorithm, increasing its efficiency by incorporating depthwise separable and refinement layers to accurately locate keypoints. To address overfitting, I applied the ADAM optimizer and L2 loss regularization, improving the model's frame rate from 30 fps to 49 fps.</p> -->

                                <!-- Detailed Description with Photos -->
                                <h3>Detailed Description</h3>
                                <p>The Double Tree RRT* algorithm uses varying edge lengths in each iteration, confining the number of maximum samples to be explored until the trees connect. This approach is achieved by applying control inputs to both trees arising from the start and goal nodes. The bidirectional exploration of the nodes from x_init and x_goal enhances the search efficiency, leading to faster convergence towards an optimal solution. The integration with SLAM using LIDAR data provides accurate map information, further improving the algorithm's performance in real-time scenarios.</p>

                                <!-- <img class="img-fluid d-block mx-auto" src="img\portfolio\Path Master Buttler RRT Lidar Integration\123.png" alt="SLAM Mapping" />
                                <p><em>Figure 1: SLAM Mapping</em></p> -->

                                <img class="img-fluid d-block mx-auto" src="img\portfolio\Path Master Buttler RRT Lidar Integration\Screenshot 2024-06-05 014916.png" alt="Path Planning" />
                                <p><em>Figure 2: Path Planning</em></p>

                                
                                <!-- Demo Videos -->
                                <h3>Demo Video</h3>
                                <iframe width="560" height="315" src="https://www.youtube.com/embed/y0VZ-XCyNpM" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

                                <!-- Link to Full Report -->
                                <h3>Read Full Report</h3>
                                <p>For more details, please refer to the full report: <a href="https://drive.google.com/file/d/1pgiz0sv4dvK8mXktW556OQd6xpeZuQ7D/view?usp=drive_link" target="_blank">Path Master-Butler-RRT-LiDAR-Integration</a></p>

                                <button class="btn btn-primary" data-dismiss="modal" type="button">
                                    <i class="fas fa-times"></i>
                                    Close Project
                                </button>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
      </div>



   

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Contact form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/agency.min.js"></script>

  </body>

</html>
